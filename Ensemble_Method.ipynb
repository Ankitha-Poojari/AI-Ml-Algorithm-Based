{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Setup & Dataset"
      ],
      "metadata": {
        "id": "OC9S35e_hYYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u2hNurDvb6W3"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data      # Features\n",
        "y = iris.target    # Labels\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "load_iris() loads the dataset (150 samples, 3 classes).\n",
        "\n",
        "We split data into training (70%) and testing (30%).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hhFBqg4vhLc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Bagging Classifier"
      ],
      "metadata": {
        "id": "ivR-YDnLhPqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Base learner: Decision Tree\n",
        "base_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Bagging: combine many trees trained on bootstrapped samples\n",
        "bagging = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bagging))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cnreIANcIgh",
        "outputId": "7b25aba3-2691-47ea-961e-8297c3947251"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Bagging = “Bootstrap Aggregating”.\n",
        "\n",
        "Each tree is trained on a random sample with replacement.\n",
        "\n",
        "Reduces variance (avoids overfitting)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3-TVe63Uhk6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Random Forest"
      ],
      "metadata": {
        "id": "zn1sMOsdhutP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Random Forest (bagging + random feature selection)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbz8244Lhcbo",
        "outputId": "200109a9-c5aa-4427-ae29-a3a6e91126f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Random Forest = Bagging + random feature subsets.\n",
        "\n",
        "Stronger than plain bagging because it decorrelates trees.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kOHyf4N-h4up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: AdaBoost"
      ],
      "metadata": {
        "id": "cTIy2J63iCyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# AdaBoost (Adaptive Boosting)\n",
        "ada = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ada = ada.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXtpu9Yjh14X",
        "outputId": "1e49b34d-aed5-444e-aaa3-2c6cc5e8c034"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Boosting = sequential training.\n",
        "\n",
        "Misclassified samples get higher weights in the next round.\n",
        "\n",
        "AdaBoost combines weak learners into a strong classifier\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AU3J4NFQiIKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Gradient Boosting"
      ],
      "metadata": {
        "id": "TUbbiuZeiLL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_3OM5eliE5q",
        "outputId": "d2a37a19-3e79-44fe-d529-7f19930fb611"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Instead of reweighting samples (like AdaBoost), Gradient Boosting fits new learners to residual errors.\n",
        "\n",
        "More flexible and powerful.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QpDbgyxhiSVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Voting Classifier"
      ],
      "metadata": {
        "id": "79PdPdbziVbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define base models\n",
        "clf1 = LogisticRegression(max_iter=200)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "clf3 = SVC(probability=True)\n",
        "\n",
        "# Voting (Hard or Soft)\n",
        "voting = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
        "voting.fit(X_train, y_train)\n",
        "\n",
        "y_pred_voting = voting.predict(X_test)\n",
        "print(\"Voting Accuracy:\", accuracy_score(y_test, y_pred_voting))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81iNuXQdiNfY",
        "outputId": "1d52d267-ed65-47c8-b271-0b352b0f765c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Hard Voting = majority rule (most votes win).\n",
        "\n",
        "Soft Voting = averages class probabilities.\n",
        "\n",
        "Combines very different models (good for diverse feature spaces).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8S91fyHYia9p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Krti728qiYRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}